# Recap
In _Module_01 - Creating a Custom Platform_ we created our custom embedded platform with the base compoments (CIPS, AI Engine, NoC, clocking infrastructure, and boot software) to create an AI Engine design stitched together.. 

In _Module_02 - Creating an AI Engine Application_ we introduced the beamforming kernel and created our downlink and uplink subgraphs, the data flow specification graph, and our top-level AI Engine application. 

In _Module_03 - Creaing the PL Kernels_ we solved the data storage problem by creating PL kernels to send reference input data to the AI Engine and store output data generated by the AI Engine. 

In this module (_Module_04 - AI Engine and PL Integration_) we stitch together the custom platform (.xpfm), the AI Engine graph (.o), and the PL Kernels (.xo) into a XCLBIN file with the v++ linker step.  


# Building the Design 
The v++ linker step for this large design takes ~16 hours to complete. We have provided a prebuilt build repo in the `support` directory for the linker step. Run the following commands to copy the prebuilt XCLBINs and XSAs into a `build` directory. 

**Use Prebuilt XCLBIN** 
TODO: update these steps to get prebuilt XSAs and XCLBINs from Xilinx.com. 
```
make  xclbin_prebuilt
```
or 

```
cp -vr support/$(BUILD_DIR) .
unzip build/rev1/hw_emu.zip -d build/rev1 
unzip build/rev1/hw.zip -d build/rev1 
```

You’ll notice that there are two revisions of the v++ linker step: rev0 and rev1. The rev0 build contains the first revision of v+ linker step that naively links the AI Engine graph design to the PL kernels and PL platform. Since the beamforming design fully utilizes PL resources, the straight-forward implementation does not meet timing. The rev1 build is the second revision of the v++ linker step with timing closure techniques applied. The XCLBINs and XSAs for both the hardware emulation (t=hw_emu) and to run on the VCK190 board (t=hw) are provided. 

**Build XCLBIN from Scatch**
Building the XCLBINs from scratch takes about 16 hours to complete. To create the the V++ linker output from scratch, you can start the builds with the following commands: 

```bash
make xclbin_all
```
or 
```
make xclbin REV=rev0
make xclbin REV=rev1
```
or 
```bash
mkdir build/rev0/hw_emu 
cd build/rev0/hw_emu
v++ -l                                                  \
    -t hw_emu                                           \
    -f ../../../Module_01_Custom_Platform/build/sw/vck190_v1_0/export/vck190_v1_0/vck190_v1_0.xpfm  \
    --save-temps                                        \
    -g                                                  \
    ../../Module_02_AI_Engine_Design/libadf.a           \
    ../../Module_03_PL_Design/ip_repo/dlbf_slave.xo     \
    ../../Module_03_PL_Design/ip_repo/dlbf_data.xo      \
    ../../Module_03_PL_Design/ip_repo/dlbf_coeffs.xo    \
    ../../Module_03_PL_Design/ip_repo/ulbf_slave.xo     \
    ../../Module_03_PL_Design/ip_repo/ulbf_data.xo      \
    ../../Module_03_PL_Design/ip_repo/ulbf_coeffs.xo    \
    --config ../../../config.ini                        \
    -o beamforming.rev0.hw_emu.xclbin

cd ../../../
mkdir build/rev1/hw_emu 
cd build/rev1/hw_emu
v++ -l                                                  \
    -t hw_emu                                           \
    -f ../../../Module_01_Custom_Platform/build/sw/vck190_v1_0/export/vck190_v1_0/vck190_v1_0.xpfm  \
    --save-temps                                        \
    -g                                                  \
    ../../Module_02_AI_Engine_Design/libadf.a           \
    ../../Module_03_PL_Design/ip_repo/dlbf_slave.xo     \
    ../../Module_03_PL_Design/ip_repo/dlbf_data.xo      \
    ../../Module_03_PL_Design/ip_repo/dlbf_coeffs.xo    \
    ../../Module_03_PL_Design/ip_repo/ulbf_slave.xo     \
    ../../Module_03_PL_Design/ip_repo/ulbf_data.xo      \
    ../../Module_03_PL_Design/ip_repo/ulbf_coeffs.xo    \
    --config ../../../config_2regslice.ini              \
    -o beamforming.rev1.hw_emu.xclbin
```

## Options 
|Switch|Description|
|  ---  |  ---  |
|-l | Run v++ in link mode to link .xo, .o, and .xpfm input files and generate an .xclbin output file.|
|-t hw_emu | Specifies hardware emulation build target.|
|-f | Specfies platform (.xpfm) file vitis will link input files to.|
|--save-temps | Saves intermediate files/directories created during the link process.|
|-g | Generates code for debugging the kernel.|
|--config \<configuration_file\>.ini| Specifies a configuration file containing more v++ switches. This file also describes how many of each PL kernel to instantiate and how to connect the PL kernels to the PLIOs in the AI Engine. The rev0 build uses the config.ini. The rev1 build uses config_2regslice.ini that contains timing closure techniques. |
|-o beamforming.rev\<0\|1\>.hw_emu.xclbin | Specifies the name of the output file generated by the v++ command. During linking process, the output files must end with `.xclbin` extension.|

## Dependencies 
|FileName|Description|
|  ---  |  ---  |
|config.ini |Configuration file used in rev0.|
|config_2regslice.ini |Configuration file used in rev1.|
|Module_01_Custom_Platform/build/.../vck190_v1_0.xpfm | The custom platform .xpfm file we built in Module 01.|
|Module_02_AI_Engine_Design/build/Work/temp/test_bf_3x.o |The AI Engine graph object file we built in Module 02.|
|Module_03_PL_Design/build/dlbf_slave.xo | The dlbf_slave PL kernel we built in Module 03.|
|Module_03_PL_Design/build/dlbf_data.xo | The dlbf_data PL kernel we built in Module 03.|
|Module_03_PL_Design/build/dlbf_coeffs.xo | The dlbf_coeffs PL kernel we built in Module 03.|
|Module_03_PL_Design/build/ulbf_slave.xo | The ulbf_slave PL kernel we built in Module 03.|
|Module_03_PL_Design/build/ulbf_data.xo | The ulbf_data PL kernel we built in Module 03.|
|Module_03_PL_Design/build/ulbf_coeffs.xo| The ulbf_coeffs PL kernel we built in Module 03.|

## Build Products 
|FileName|Description|
|  ---  |  ---  |
|build/rev\<0\|1\>/\_x | Dictory that stores intermediate files/directories created during the link process|
|build/rev\<0\|1\>/beamforming.hw_emu.xclbin | The output file .xclbin created from the v++ linker step.|
|build/rev\<0\|1\>/beamforming.hw_emu.xclbin.info ||
|build/rev\<0\|1\>/beamforming.hw_emu.xclbin.link_summary | The Link Summary report is created by the v++ command during linking and creation of the .xclbin file|
|build/rev\<0\|1\>/beamforming.hw_emu.xsa | New hardware platform.|
|build/rev\<0\|1\>/beamforming.hw_emu.xclbin.log | Output log of the v++ -l command|


# Introduction: Linking the System
After the AI Engine graph is created and RTL kernels are packaged, the `v++ --link` command links them with the target platform to build the device binary (xclbin). This XCLBIN is used to program the hardware includes the PDI, Debug data, Memory topology, IP Layout, Metadata. 

# Timing Failure 
Open the timing summary reports from rev0 and rev1. 
```
support/rev0/hw/_x/link/vivado/vpl/prj/prj.runs/impl_1/vck190_v1_0_wrapper_timing_summary_routed.rpt
support/rev1/hw/_x/link/vivado/vpl/prj/prj.runs/impl_1/vck190_v1_0_wrapper_timing_summary_routed.rpt
```
insert screenshot of rpts and negative slack 

The rev0 build has a negative slack and fails timing while the rev1 build does not. During the v++ linker step, the rev0 build used the config.ini file with no timing closure techniques options. The rev1 build used the config_2regslice.ini with timing closure options applied. First we’ll go over the rev0 build's config.ini file and then see what changes needed to done in rev1 build to meet timing.  

# Configuration File (config.ini)
The Vitis compiler has three sections in the configuration file (config.ini): `[connectiviy]`, `[clock]`, and `[advanced]`. Open the `config.ini` file, review the comments, and follow along on this readme. 

## \[connectivity\] Section
The \[connectivity\] section includes the `nk` and `sc` switches. 

### Number of Kernels
**`nk`:** specifies the number of kernels instances or CU the v++ command adds to the device binary (XCLBIN). It specifies the kernel name, the number of instances (or CUs) of that kernel, and the CU name for each instance. For example, the first set of kernel instances is the following:

```
#PL Kernel Instances for Downlink Subgraph 0
nk=dlbf_data:1:dlbf_data_00
nk=dlbf_coeffs:8:dlbf_coeffs_00.dlbf_coeffs_01.dlbf_coeffs_02.dlbf_coeffs_03.dlbf_coeffs_04.dlbf_coeffs_05.dlbf_coeffs_06.dlbf_coeffs_07
nk=dlbf_slave:8:dlbf_o00.dlbf_o01.dlbf_o02.dlbf_o03.dlbf_o04.dlbf_o05.dlbf_o06.dlbf_o07
```

The first line specifies there is one `dlbf_data` PL kernel named `dlbf_data_00`. The second line specifies there are 8 `dlbf_coeffs` PL kernels name `dlbf_coeffs_00 ... dlbf_coeffs_07`. The last line specifies there are 8 `dlbf_slave` PL kernels named `dlbf_o00 ... dlbd_o07`. 

The config.ini files repeats this 3 times since there are three instances of the downlink subgraph. 

The config.ini file repeats this connection scheme for the ULBF PL kernels: 
```
#PL Kernel Instances for Uplink Subgraph 0
nk=ulbf_data:1:ulbf_data_00
nk=ulbf_coeffs:4:ulbf_coeffs_00.ulbf_coeffs_01.ulbf_coeffs_02.ulbf_coeffs_03
nk=ulbf_slave:4:ulbf_o00.ulbf_o01.ulbf_o02.ulbf_o03
```
The config.ini files repeats this 3 times since there are three instances of the uplink subgraph. Each uplink subgraph requires 1 ulbf_data kernel, 4 ulbf_coeff kernels, and 4 ulbf_slave kernels. 

### Streaming Connections 
**`sc`:** defines connections between the ports of the AI Engine graph and the streaming ports of PL kernels. The first set of `sc` connections are between the `dlbf_data` PL kernel instances and the AI Engine `dlbfid` (input data) ports. The next set of connections are between the `dlbf_coeffs` PL kernels and the AI Engine `dlbfic` (input coefficient) ports. The AI Engine ports are connected to the PL kernels’ `M00_AXIS` (master AXI4-Stream) ports.

The next set of connections are between the AI Engine `dlbfo` (output) ports and the `dlbf_o_**` (`dlbf_slave` instances). The AI Engine ports are connected to the PL kernels’ `S_AXIS` (AXI4-Stream) ports. 

The `config.ini` file repeats this connection scheme for all 3 instances of the DLBF system and for all 3 ULBF systems as well. 

## \[clock\] Section
The next section of the `config.ini` is the `[clock]` section. Notice it only uses two clocks (`id=0` and `id=4`). The clock `id=0` is a 100MHz clock. Clock `id=4` is a 400MHz clock. These clocks were created by the clocking wizard in the custom platform (_Module 01 - Creating a Custom Platform_). They are available for the V++ linker to use in this step. This is possible because we exposed them using the `PFM.clock` function when creating the custom platform (.xfpm). 

All of the PL kernels’ `s_axi_aclk` is connected to the 100MHz clock (`id=0`). This clock is for the AXI4-LITE port of the PL kernels. It clocks the control path in the PL kernels which allows the CIPS to send/receive control and status signals from the PL kernels. The `dlbf_data`, `ulbf_data`, `dlbf_coeff`, and `ulbf_coeff` PL kernels have a `m_axis_clk`. The `dlbf_slave` and `ulbf_slave` PL kernels have a `s_axis_clk`. These clocks are connected to the 400MHz clock (`id=4`). This clock is for the AXI4-Stream port on these PL kernels. It clocks the data path in the PL kernels which allows the AI Engine to receive or send data to/from the PL kernels.

## \[advanced\] Section

**`param=compiler.addOutputTypes=hw_export`:** specifies the creation of a new XSA for the target platform. The exported XSA has the name of the output file specified by the `-o` option in the V++ command, with the extension `.xsa`. This advanced option should only be set when creating a new XSA. This option is required for building the fixed platform in the baremetal flow.

**`param=hw_emu.enableProfiling=false`:** sets the hardware emulation profiling feature off. 

**`param=compiler.maxComputeUnits=1000`:** sets the maximum number compute units allowed in the system. 

## New XSA Platform: rev0
After the v++ linker step is complete, you can open the block design in the Vivado project to view the new hardware platform. 
Open the block design in the Vivado project to view the new hardware platform at `build/rev0/hw_emu/_x/link/vivado/vpl/prj/prj.xpr`. 

![Linked Hardware Platform](images/linker_vivado_block_diagram.png) 

Notice the new XSA hardware platform built on top of the custom platform we built in _Module-01-Creating a Custom Platform_. It still contains the original building blocks: CIPS, NoC, AI Engine, Clocking Wizard, Processor Reset Systems, ctrl_sm, and 16 AXI4-LITE SmartConnect Interfaces. However, you'll notice only six of them are being used and ten of them remain unused. 

Each AXI4-LITE SmartConnect Interface can have upto 15 AXI4-LITE Master Interfaces. Four of them have 15 AXI4-LITE Master interfaces instantiated each. One of them has 14 AXI4-LITE Master interfaces instantiated. And one of them has four AXI4-LITE Master interfaces instantiated. This is total of 78 AXI4-LITE Master interfaces which are connected to the newly linked PL kernels (3 dlbf_data, 24 dlbf_coeff, 3 ulbf_data, 12 ulbf_coeff, 24 dlbf_slave, and 12 ulbf_slave kernels). The AI Engine is also connected to all the PL kernels through their AXI4-Stream interfaces.

# Timing Closure
we know that the straightforward configuration file for the v++ linker didn't meet timing. This was because TODO why didn't it meet timing? This is solved by adding two axi4s-register slices between the PL kernels and the AI Engine. 

# Configuration File (config_2regslice.ini)
Now we'll review the configuration file that contains timing closure techniques that created rev1 which did meet timing. Open the `config_2regslice.ini` file, review the comments, and follow along on this readme. 

## \[connectivity\] Section
You'll notice that there's an additional nk switch added is creates 528 axi4s_regslice PL kernels. 

```
nk=axi4s_regslice_64b:528
```
The sc switches are also altered so that there are two axi4s_regslice_64b kernels between the original PL kernels and AI Engine. Below is an example of how this is done. 
```
sc=dlbf_data_00.M00_AXIS:axi4s_regslice_64b_1.S_AXIS
sc=axi4s_regslice_64b_1.M_AXIS:axi4s_regslice_64b_2.S_AXIS
sc=axi4s_regslice_64b_2.M_AXIS:ai_engine_0.dlbfid0
```

## \[clock\] Section
All the axi4s_regslice-64b kernels are clocked by id=3 (400MHz). 

## \[vivado\] Section
At the end of the config_2regslice.ini file, you'll notice there are additional vivado options given to the v++ linker. 

**`prop=run.impl_1.STEPS.ROUTE_DESIGN.ARGS.MORE OPTIONS=-tns_cleanup`:**

**`param=project.writeIntermediateCheckpoints=1`:**

**`prop=run.impl_1.STEPS.PLACE_DESIGN.ARGS.DIRECTIVE=ExtraNetDelay_high`:**

**`prop=run.impl_1.STEPS.PHYS_OPT_DESIGN.ARGS.DIRECTIVE=AggressiveExplore`:**

**`prop=run.impl_1.STEPS.ROUTE_DESIGN.ARGS.DIRECTIVE=AggressiveExplore`:**

## New XSA Platform: rev1 
Open the block design of rev1. Notice how the axi4s_regslice_64b kernels are inserted between the AI Engine and original PL kernels. 

insert screenshot 


# References 
UG1076 - Chapter 13 Linking the System 

UG1416 Vitis Unified Software Documentation --> Application Acceleration Development --> Linking the Kernels 

UG1416 Vitis Compiler Configuration File

UG1416: Creating Multiple Instances of a Kernel 

UG1416: Connectivity Options 

Vitis Application Acceleration Development Flow Documentation https://www.xilinx.com/html_docs/xilinx2020_1/vitis_doc/kme1569523964461.html 
 
Vitis compiler options https://www.xilinx.com/html_docs/xilinx2020_1/vitis_doc/vitiscommandcompiler.html

Vitis hw_emu vs hw build target https://www.xilinx.com/html_docs/xilinx2020_1/vitis_doc/buildtargets1.html#rst1525720251890
