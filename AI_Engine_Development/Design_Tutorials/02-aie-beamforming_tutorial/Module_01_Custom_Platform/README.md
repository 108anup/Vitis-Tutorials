# Building the Design
Create the custom platform by running the `make` step as follows. It will create a Vivado project from the `vck_190_v1_0_xsa.tcl` provided and the Vitis baremetal embedded project from the `vck190_v1_0_pfm.tcl` provided. 

```bash
make platform
```

or 

```bash 
cd hw
vivado -mode batch -source vck190_v1_0_xsa.tcl -tclargs xcvc1902-vsva2197-1LHP-i-L
cd ../sw
xsct -nodisp -sdx ./vck190_v1_0_pfm.tcl vck190_v1_0 ../hw/vck190_v1_0.xsa build
```
## Options Table

|Switch|Description|
|  ---  |  ---  |
|-mode batch -source vck190_v1_0_xsa.tcl| Vivado Design Suite Tcl shell will open, run the specified Tcl script, and exit when the script completes|
|-tclargs xcvc1902-vsva2197-1LHP-i-L|The -tclargs option allows you to pass arguments for the Tcl script you are running. The vck190_v1_0_xsa.tcl requires you to specify in the device-package name.|
|-nodisp|No display|
|-sdx ||
|./vck190_v1_0_pfm.tcl vck190_v1_0 ../hw/vck190_v1_0.xsa build | TCL script that takes in three arguments: platform name, xsa file, and output path|

## Dependencies

|Filename|Description|
|  ---  |  ---  |
|build/hw/vck190_v1_0_xsa.tcl|Creates Vivado project and exports platform.|
|build/hw/dr.bd.tcl|Creates the block design in the platform Vivado project.|
|build/hw/pfm_decls.tcl|Defines clock IDs to output clock generated by clocking wizard.|
|build/hw/ddr4_dmc_3200_triplet_1.xdc|DDR4 memory constraints.|
|build/hw/qor_scripts/pre_place.tcl|Calls the prohibitCascDspAcrossRbrk.tcl, prohibitCascBramAcrossRbrk.tcl, and prohibitCascUramAcrossRbrk.tcl before placement in the platform Vivado project.|
|build/hw/qor_scripts/prohibitCascDspAcrossRbrk.tcl|Prohibit cascading of DSP resources to close timing.|
|build/hw/qor_scripts/prohibitCascBramAcrossRbrk.tcl|Prohibit cascading of BRAM_18K and BRAM_36K resources to close timing.|
|build/hw/qor_scripts/prohibitCascUramAcrossRbrk.tcl|Prohibit cascading of URAM resources to close timing.|
|build/vck190_v1_0_pfm.tcl|Create a new platform by importing XSA. Creates and configures an aiengine, xrt, and standalone domain in the platform.|
|build/sw/src/qemu/aie/pmc_args.txt|File with all pmcqemu args listed. This is used to start pmcqemu.|
|build/sw/src/qemu/aie/qemu_args.txt|File with all PS QEMU args listed. This is used to start PS QEMU.|
|build/sw/src/boot/|Directory which has all the files listed in file-names provided as part of qemu-args and pmuqemu-args options. It is the directory to generate components after Linux Image build.|
|build/sw/src/a72/xrt/image|For domain with Linux as OS, use pre-built Linux images from this directory, while creating the PetaLinux project. This option is valid only for Linux domains.|
|build/sw/src/a72/xrt/linux.bif|Bif file used to create boot image for Linux boot.|
|build/sw/src/qemu/lnx/pmc_args.txt|File with all pmcqemu args listed. This is used to start pmcqemu|
|build/sw/src/qemu/lnx/qemu_args.txt|File with all PS QEMU args listed. This is used to start PS QEMU.|

## Build Products

|Filename|Description|
|  ---  |  ---  |
|build/hw/build/NOC_Power.xpe|Generated from the Vivado command. Something about power.|
|build/hw/build/vck190_v1_0.xsa|Hardware platform exported from Platform Vivado project. Also known as hardware specification file.|
|build/hw/build/vck190_v1_0_vivado/|Platform Vivado project folder.|
|build/hw/build/vivado.log| Vivado log. |
|build/hw/build/vivado.jou| Vivado journal. |
|build/sw/build/vck190_v1_0|Generated from the XSCT command. Contains the final platform file vck190_v1_0.xpfm used by Vitis later on.|

# Introduction: What is a Custom Vitis Embedded Platform?
A platform is the starting point of any Vitis design. Vitis applications are built on top of the platforms. Xilinx provides pre-built base platforms on the Xilinx download center: https://www.xilinx.com/support/download.html. The source code of these base platforms is available via the Xilinx GitHub site: https://github.com/Xilinx/Vitis_Embedded_Platform_Source.

In this beamforming tutorial, we created a custom Vitis embedded platform from scratch. An embedded platform includes a hardware platform and a software platform. 

## Hardware Platform
The hardware platform is the static, unchanging portion of your hardware design. It includes the Xilinx Support Archive (XSA) file exported from the Vivado Design Suite. The hardware platform describes platform hardware setup and the acceleration resources that can be used by acceleration applications, for example, input and output interfaces, clocks, and AXI buses. Vitis builds on top of the platform to connect up the user’s PL kernels and AI Engine design. 

The XSA for the beamforming tutorial was generated by the vivado command:
```
vivado -mode batch -source vck190_v1_0_xsa.tcl -tclargs xcvc1902-vsva2197-1LHP-i-L
```

## Software Platform
The software platform is the environment that runs the software to control the kernels for acceleration applications. It includes the domain setup and boot components setup. The software platform in the beamforming tutorial has three domains: an AI Engine domain, a Linux domain, and a baremetal (standalone) domain. The components to builds the AI Engine and Linux domains are provided in the `sw_comp` folder. 

The software platforms and final xpfm platform is generated by the xsct command:
```
xsct -nodisp -sdx ./vck190_v1_0_pfm.tcl vck190_v1_0 hw output
```

# Hardware Platform 
The Platform Vivado project is generated with the tcl script `vck_190_v1_0_xsa.tcl`. Open this script, review the comments, and follow along on the rest of this readme. 

**Create Platform Vivado Project**

The first thing this `vck_190_v1_0_xsa.tcl` script does is call the `create_project` function and create the Vivado project named `vck190_v1_0_vivado`. 

**Create Block Design**

Then the `vck_190_v1_0_xsa.tcl` script calls the `dr.bd.tcl` script which generates the platform block design. Open the block design, and you’ll notice there are five major components to the hardware platform: the AI Engine, CIPS, NoC, Clocking Wizard IP, Clock Reset IPs, 16 AXI4-LITE SmartConnect Interfaces, and a Top-Level AXI4-LITE SmartConnect (called ctrl_sm). 

![Platform Block Design](images/Module_01_Custom_Platform_block_design.png)

Open the `dr.bd.tcl` file and review the comments. After the `Setup and Error Checking` commands, you’ll notice the main function of this file is called `create_root_design`. 

**Port Instantiation**

The first thing the `create_root_design` function does is it creates two block design interface ports called `SYS_CLK1_IN_0` and `ddr4_dimm1` for the system clock and DDR memory ports.  

**AI Engine**

Next, the `dr.bd.tcl` file creates an instance of the AI Engine and sets its properties. Notice how most of the ports are empty (`{}`). Here, we’re just instantiated the AI Engine interfaces in the platform. Later on, the Vitis Linker step will connect the AI Engine to the PL kernels. At this step, the AI Engine’s `S00_AXI` slave interface configuration category is set to NoC. 

**AXI Debug Hub IP and Simulation Clock and Reset Generator IP**

Next, an instance of the AXI Debug Hub IP and an Simulation Clock and Reset Generator IP is created and their properties are set. 

**AXI SmartConnects**

Next, the `dr.bd.tcl` creates an instance of an AXI SmartConnect IP called `ctrl_sm`. This is the top-level AXI SmartConnect which is configured to have one clock, 16 master interfaces, and one slave interface. After this, there are 16 additional AXI SmartConnect IPs instantiated. We will connect these to the 16 master interfaces in the `ctrl_sm` later on in the script. 

**AXI Verification IPs**

Next we create 16 instances of the AXI Verification IP and set their properties. Later on in the script, we will connect the 16 AXI SmartConnect IPs to the 16 AXI Verification IPs.

**Clock Infrastructure**

Next, we create seven instances of the Processor Reset System IP and an instance of the Clocking Wizard. The Clocking Wizard is configured to generate seven output clocks (i.e. `CLOCKOUT_REQUESTED_OUT_FREQUENCY`= 100MHz, 250MHz, 500MHz, 400MHz, 450MHz, 250MHz, and another 100MHz). We need a Process Reset System IP for each output clock. 

**CIPS**

Next, we instantiate and configure the CIPS IP block. Notice the system monitor is enabled with the use of the `CONFIG.SMON_*` properties. 

**NoC**

Next, the NoC is instantiated and its properties are set. It has two AXI master interfaces, eight AXI slave interfaces, and 10 clocks. The NoC configuration is as follows: 

|NoC Interface|Category| NoC Clock|
|  ---   |  ---      |  ---  |
|M00_AXI | PL        | aclk5 |
|M01_AXI | AI Engine | aclk9 | 
|S00_AXI | PS CCI    | aclk0 |
|S01_AXI | PS CCI    | aclk1 |
|S02_AXI | PS CCI    | aclk2 |
|S03_AXI | PS CCI    | aclk3 |
|S04_AXI | PS PMC    | None  |
|S05_AXI | PS NCI    | aclk6 |
|S06_AXI | PS NCI    | aclk8 |
|S07_AXI | PS RPU    | aclk7 |

**Create Interface Connections**

Now that the major components of the block design are instantiated, the next step is to connect the all together with the `connect_bd_intf_net` commands. 

**Clock Connections** 
The first connection we created is the connecting the `SYS_CLK1_IN_0_1` port we created at the beginning of the `dr.bd.tcl` script to connect with the Simulation Clock and Reset Generator IP’s `SYS_CLK0_IN` input port. 

Then we connect Simulation Clock and Reset Generator IP’s `SYS_CLK0` pin is connected to the NoC’s `sys_clk0` pin. 

**AXI SmartConnects Connections**

The next set of connection we will make will be the AXI SmartConnect connection. First, we connect the 16 AXI Verification IP masters to the 16 AXI SmartConnects slaves. Then, we connect the 16 AXI SmartConnect to the 16 master interfaces in the top-level AXI SmartConnect (sm_ctrl). Then we connect the `sm_ctrl` to the master interface `M_AXI_FPD` on the CIPs. These connections allow any of the master AXI Verification IPs to access the CIPS slave connected to the `sm_ctrl` AXI SmartConnect.   

**CIPS and NoC Connections** 

The next set of connections we will make will be between the CIPS and the NoC. The CIPS connections were added to the previous NoC interface table to illistrate the connections being created.  

|NoC Interface|Category|NoC Clock|Connection Interface|
|  ---   |  ---      |  ---  |  ---  |
|M00_AXI | PL        | aclk5 ||
|M01_AXI | AI Engine | aclk9 || 
|S00_AXI | PS CCI    | aclk0 | ps_cips/FPD_CCI_NOC_0 | 
|S01_AXI | PS CCI    | aclk1 | ps_cips/FPD_CCI_NOC_1 |
|S02_AXI | PS CCI    | aclk2 | ps_cips/FPD_CCI_NOC_2 |
|S03_AXI | PS CCI    | aclk3 | ps_cips/FPD_CCI_NOC_3 |
|S04_AXI | PS PMC    | None  | ps_cips/PMC_NOC_AXI_0 |
|S05_AXI | PS NCI    | aclk6 | ps_cips/FPD_AXI_NOC_0 |
|S06_AXI | PS NCI    | aclk8 | ps_cips/FPD_AXI_NOC_1 |
|S07_AXI | PS RPU    | aclk7 | ps_cips/FPD_LPD_AXI_0 |


**NoC Connections**

The next set of connections will be connect the NoC up to other parts of the design. First, the `ddr4_dimm1` port we created at the beginning of the `dr.bd.tcl` script is connected to the NoC’s `CH0_DDR4_0` interface. This configures the NoC to have a Single Memory Controller port. 

Then we connect the AXI Debug Hub IP to the NoC's master interface ( `M00_AXI`). Then we connect the AI Engine to the NoC's second master interface (`M01_AXI`). We also set the AI Engine’s `s00_axi_aclk` to the NoC’s `aclk9`. 

|NoC Interface|Category|NoC Clock|Connection Interface|
|  ---   |  ---      |  ---  |  ---  |
|M00_AXI | PL        | aclk5 |axi_dbg_hub_0/S_AXI|
|M01_AXI | AI Engine | aclk9 |ai_engine_0/S00_AXI| 
|S00_AXI | PS CCI    | aclk0 | ps_cips/FPD_CCI_NOC_0 | 
|S01_AXI | PS CCI    | aclk1 | ps_cips/FPD_CCI_NOC_1 |
|S02_AXI | PS CCI    | aclk2 | ps_cips/FPD_CCI_NOC_2 |
|S03_AXI | PS CCI    | aclk3 | ps_cips/FPD_CCI_NOC_3 |
|S04_AXI | PS PMC    | None  | ps_cips/PMC_NOC_AXI_0 |
|S05_AXI | PS NCI    | aclk6 | ps_cips/FPD_AXI_NOC_0 |
|S06_AXI | PS NCI    | aclk8 | ps_cips/FPD_AXI_NOC_1 |
|S07_AXI | PS RPU    | aclk7 | ps_cips/FPD_LPD_AXI_0 |

**Clocking Infrastructure Connections**

The next set of connections will be the connect the output clocks of the Clocking Wizard to the rest of the design. First, we connect the Clocking Wizard's output clock 6 (100MHz) to the AXI Debug Hub IP and the NoC PL clock (`aclk5`). We also sync the Processor Reset System #6 to this clock as well. We connect the AXI Debug Hub IP’s asynchronous reset to the Procesosr Reset System #6 as well. 


Next, we connect the Clocking Wizards output clock 1 (250MHz) to the ctrl_sm, the 16 AXI SmartConnects, 16 AXI Verification IPs, and CIPS' `m_axi_fpd_aclk`. We also sync the Processor Reset System #1 to this clock as well.


Next, we sync the rest of the Processor Reset Systems (#2, 3, 4, and 5) to the Clocking Wizard's output clocks (clock output 2=500MHz, clock output 3=400MHz, clock output 4=450MHz, and clock output 5=250MHz). 

Next, we connect the Clocking Wizard’s `locked` pin to the Processor Reset Systems’ `dcm_locked` pins. 

Next, we connect the CIPS’ `pl0_ref_clk` as the input to the Clocking Wizard (`clk_in1`). 

Lastly, we connect the CIPs’ `pl0_resetn` as the input to the Processor Reset Systems’ `ext_reset_in` pins. 

**CIPS Clocks**

The last set of connections will be to connect the CIPS clock pins to the NoC clock pins. A summary table of the connections are provided below: 

|NoC Interface|Category|NoC Clock|Connection Interface|CIPS Clock|
|  ---   |  ---      |  ---  |  ---  |  ---  |
|M00_AXI | PL        | aclk5 |axi_dbg_hub_0/S_AXI||
|M01_AXI | AI Engine | aclk9 |ai_engine_0/S00_AXI||
|S00_AXI | PS CCI    | aclk0 | ps_cips/FPD_CCI_NOC_0 |fpd_cci_noc_axi0_clk| 
|S01_AXI | PS CCI    | aclk1 | ps_cips/FPD_CCI_NOC_1 |fpd_cci_noc_axi1_clk|
|S02_AXI | PS CCI    | aclk2 | ps_cips/FPD_CCI_NOC_2 |fpd_cci_noc_axi2_clk|
|S03_AXI | PS CCI    | aclk3 | ps_cips/FPD_CCI_NOC_3 |fpd_cci_noc_axi3_clk|
|S04_AXI | PS PMC    | None  | ps_cips/PMC_NOC_AXI_0 ||
|S05_AXI | PS NCI    | aclk6 | ps_cips/FPD_AXI_NOC_0 |fpd_axi_noc_axi0_clk|
|S06_AXI | PS NCI    | aclk8 | ps_cips/FPD_AXI_NOC_1 |fpd_axi_noc_axi1_clk|
|S07_AXI | PS RPU    | aclk7 | ps_cips/FPD_LPD_AXI_0 |lpd_axi_noc_clk|


**Create Address Segments**

After making the connections between the blocks, we next assign memory addresses in CIPS to various blocks (AI Engine, AXI Debug Hub, 16 AXI SmartConnects, NoC).

**Set Platform Attributes**

Next, we need to set the Platform properites of the Vivado design (i.e. properties PFM.\*). First, we set the `PFM_NAME` to the current block design. 

Next, we need to add the control and memory hardware interfaces with the `PFM.AXI_PORT` function. 

**Control Interfaces Requirements**

Every platform must declare at least on general purpose AXI master port (`M_AXI_GP`). You’ll see that for each of the 16 AXI SmartConnects, we have declared 15 `M_AXI_GP` interfaces. Additionally, there is a `M_AXI_NoC` control interface on the NoC. These interfaces are used by the Vitis linker step to connect the PL kernels to the platform. 

**Memory Interfaces Requirements**

Every platform must declare at least one memory interface with the AXI Slave port (S_AXI_\*). You’ll see that there are 28 `S_AXI_NOC` memory interfaces declared. The Vitis linker step will connect DDR4 memory to these ports.  

**Clock Requirements**

Every platform must have at least one clock enabled in `PFM.CLOCK` property and one clock must be set to default. You’ll see that there are five clocks declared and `clk_out1` is the default. 

**Set Platform Attributes with `for` Loops**

After creating the block diagram, the `vck_190_v1_0_xsa.tcl` script sources the `pfm_decls.tcl` script. This script reapplies the `PFM.*` properties, but showcases how to do it with `for` loops. 

**DDR4 Constraints**

After sourcing the `pfm_decl.tcl` script, the `vck_190_v1_0_xsa.tcl` script sets the DDR4 design constraints. Open the `ddr4_dm_3200_triplet-1.xcd` constraints file. It sets the package pins to power to the correct DDR4 banks in use. 

**Create Wrapper for Block Design**

Next the `vck_190_v1_0_xsa.tcl` calls the `make_wrapper` function to create a wrapper for the block design and set it as the top-level wrapper.

**Post Link TCL Commands**

Some platforms require post link tcl commands to complete the platform creation. The beamforming design did not require any post link tcl commands, therefore this script is empty. The script is provided for you, if you choose to modify the beamforming platform and add any post link tcl commands to the platform vivado project. 

**Timing Closure**

After a Vivado design is created, Vivado then go through the design implementation process. This process involves placement, routing, and physical optimization. 

* Placement is placing the specified ports and logic cells onto device resources. 
* Routing routes the nets in the design to complete logic connections on the target part. 
* Physical optimization performs timing-driving optimization on negative-slack paths of a design.  

Large platforms often require tcl commands before and after these design implementation steps to guide the Vivado to a timing closed implementation. 

The next part in the `vck_190_v1_0_xsa.tcl` sets the `place_design`, `route_design`, and `phys_opt_design` pre and post tcl hooks to specific scripts that help close timing during implementation. 

The `pre_place.tcl` script calls the `prohibitCascBramAcrossRbrk.tcl`, `prohibitCascUramAcrossRbrk.tcl`, and `prohibitCascDspAcrossRbrk.tcl` scripts which prohibit the cascading the BRAM, URAM, and DSP resources. It also calls the `waive_BLI_AIE_timing_violations_preplace.tcl` script which allows the timing violations between the BLI registers and AI Engine before placements. 

The `post_place.tcl` script calls the `waive_BLI_AIE_timing_violations_postplace.tcl` which allows timing violations between the BLI register and the AI Engine after placement. 

The `post_route.tcl` and the `post_physopts.tcl` scripts are empty since beamforming tutorial did not require any tcl commands after routing and physical optimization. These scripts are provided for users to add their commands if they wanted to modify this tutorial. 

**Emulation Setup**

The next step the `vck_190_v1_0_xsa.tcl` script does is setup the platform for hardware emulation. When creating emulation-capable platforms, CIPS and NoC must use TLM (transaction-level modeling) as the simulation model.  

**Platform Output Type**

The next step the `vck_190_v1_0_xsa.tcl` script does is set the platform output type to “hw_export”. 

**Wrap Up Vivado Project**

The next few commands in  the `vck_190_v1_0_xsa.tcl` script finalize and wrap up the Vivado project creation process by updating the compiler order, assigning the block design addresses, and validating the block design.

**Export Hardware XSA**

Lastly, the `vck_190_v1_0_xsa.tcl` script generates the files neccarry to support the block design with the `generate_target` command and write the pre-synthesis expandable XSA with the `write_hw_platform` command. 
 
## Software Platform
After creating the hardware platform (XSA) with the AI Engine IP through Vivado, we will import this platform into Vitis and create the software platform. Then we'll add the aie_runtime, Linux, and standalone (baremetal) domains into the platform. In this section we will review important aspects of the `vck_190_v1_0_pfm.tcl` script used to generate the Vitis project. Open this script, review the comments, and follow along on the rest of this readme. 

**Platform Create**

The first step `vck_190_v1_0_pfm.tcl` executes is creating the software platform and setting the platform project name, description, XSA and output path using the `platform create` command. The command mark the platform to build without generating boot components with the -no-boot-bsp option. 

```tcl
platform create 
  -name $platform_name              \
  -desc " A platform targetting VCK190 for demonstration purpose with a Linux, AI Engine and a Standalone domain" \
  -hw $xsa_path/$platform_name.xsa  \ 
  -out $output_path                 \
  -no-boot-bsp
```

**Domain Create: AI Engine**

Next, `vck_190_v1_0_pfm.tcl` script creates an AI Engine domain and selects the `aie_runtime` as the Operating System and `ai_engine` as the processor with the `domain create` command. It configures the `pmcqemu-args`, `qemu-args`, and `qemu-data` attributes with the `domain config` command.  

```tcl
domain create -name aiengine -os aie_runtime -proc {ai_engine}
domain config -pmcqemu-args $SW_COMP/src/qemu/aie/pmc_args.txt
domain config -qemu-args $SW_COMP/src/qemu/aie/qemu_args.txt
domain config -qemu-data $SW_COMP/src/boot
```

**Domain Create: Linux**

Next, the `vck_190_v1_0_pfm.tcl` script creates creates a Linux domain and selects `linux` as the Operating System and the `psv_cortexa72` as the processor with the `domain create` command. This domain uses the pre-build XRT Linux image when creating the Petalinux project. It sets the `boot`, `bif`, `pmcqemu-args`, `qemu-args`, and `qemu-data` with the `domain config` command. 

#### BIF File
The BIF file describes the boot components and their properties for Bootgen to generate the boot.bin file. A BIF file must be provided here so taht the application build process can package the boot image. 

#### Boot Directory 
The boot directory includes all the files described in the BIF file must also be provided. The contents of the Image directory will be copied in the FAT32 partition of the final SD card image. In the beamforming example, the boot directory contains the bl31.elf, and u-boot.elf, and system.dtb files generated by Petalinux. Later on, Vitis compiler (v++) looks for files in the boot directory and replaces the placeholders with real file names nad pahts. It then calls Bootgen to generate the BOOT.BIN. 

```tcl
## Create the Linux domain
domain create -name xrt -proc psv_cortexa72 -os linux -image $SW_COMP/src/a72/xrt/image
domain config -boot $SW_COMP/src/boot
domain config -bif $SW_COMP/src/a72/xrt/linux.bif
domain config -pmcqemu-args $SW_COMP/src/qemu/lnx/pmc_args.txt
domain config -qemu-args $SW_COMP/src/qemu/lnx/qemu_args.txt
domain config -qemu-data $SW_COMP/src/boot
```

**Domain Create: Baremetal**
Lastly, the `vck_190_v1_0_pfm.tcl` script creates the baremetal domain and selects `standalone` as the operating system and the `psv_cortexa72_0` as the processor with the `domain create` command.

```
## Create the Standalone domain 
domain create -name standalone_domain -os standalone -proc psv_cortexa72_0
```

### Generate Platform
Finally, software platform is generated with the `platform generate` command. The final generated custom platform can be found in `build/output/vck190_v1_0/export/vck190_v1_0/vck190_v1_0.xpfm`. This is the platform that will be used in later modules and Vitis will build on top of. 

# References
[Creating Embedded Platforms in Vitis](https://www.xilinx.com/html_docs/xilinx2020_2/vitis_doc/create_embedded_platforms.html#rjs1596051748503)

[Vitis Tutorials: Platform Creation](https://github.com/Xilinx/Vitis-Tutorials)

[UG835 Vivado Design Suite Tcl Command Reference Guide](https://www.xilinx.com/support/documentation/sw_manuals/xilinx2020_2/ug835-vivado-tcl-commands.pdf)

[Xilinx Software Command-Line Tool (XSCT)](https://www.xilinx.com/html_docs/xilinx2020_2/vitis_doc/jed1590410655455.html)

[AI Engine Documentation](https://www.xilinx.com/html_docs/xilinx2020_2/vitis_doc/yii1603912637443.html)

[Internal Only Create Baremetal Platform](https://confluence.xilinx.com/display/~florentw/AI+Engine+A+to+Z+-+Custom+Baremetal+Base+Platform+Creation+%282020.1%29+-+Part+1+-+Vivado)

[AI Engine Product Guide PG358](https://www.xilinx.com/support/documentation/ip_documentation/ai_engine/v2_0/pg358-versal-ai-engine.pdf)

[AXI Debug Hub](https://www.xilinx.com/products/intellectual-property/axi_dbg_hub.html#overview) 

[Simulation Clock and Reset Generator IP](https://www.xilinx.com/products/intellectual-property/sim-rst-gen.html)

[AXI SmartConnect](https://www.xilinx.com/products/intellectual-property/smartconnect.html)

[AXI Verification IP](https://www.xilinx.com/products/intellectual-property/axi-vip.html)

[Clocking Wizard](https://www.xilinx.com/products/intellectual-property/clocking_wizard.html)

[Processor Reset System Module](https://www.xilinx.com/products/intellectual-property/proc_sys_reset.html)

[CIPS Product Guide](https://www.xilinx.com/support/documentation/ip_documentation/versal_cips/v2_1/pg352-cips.pdf)

[Versal ACAP Programmable Network on Chip and Integrated Memory Controller v1.0 PG313](https://www.xilinx.com/support/documentation/ip_documentation/axi_noc/v1_0/pg313-network-on-chip.pdf) 



